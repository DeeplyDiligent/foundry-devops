# Evaluation configuration for hello-world agent
name: hello-world-quality-evaluation
description: Evaluate hello-world agent responses for quality metrics

# Evaluation mode: 'dataset' or 'agent'
mode: agent

# Agent configuration (required for agent mode)
agent:
  name: hello-world
  # version: "1"  # Optional - will use latest if not specified

# Test data
data:
  file: evaluations/evaluation-data/data-sample.jsonl

# Evaluators to run
evaluators:
  # Built-in evaluators
  - name: builtin.relevance
    type: builtin
    parameters:
      deployment_name: gpt-4.1
      threshold: 0.7
  
  - name: builtin.coherence
    type: builtin
    parameters:
      deployment_name: gpt-4.1
      threshold: 0.7
  
  - name: builtin.groundedness
    type: builtin
    parameters:
      deployment_name: gpt-4.1
      threshold: 0.7

# Output configuration
output:
  directory: evaluations/results
  format: json
